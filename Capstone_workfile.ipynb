{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D , Dropout, MaxPooling2D, Flatten, Dense\n",
    "from PIL.Image import core as image\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see that GPU is being used\n",
    "# import tensorflow as tf\n",
    "# tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong Shape Image Count: 0\n",
      "Image Count: 54304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Apple___Apple_scab': 630,\n",
       " 'Apple___Black_rot': 621,\n",
       " 'Apple___Cedar_apple_rust': 275,\n",
       " 'Apple___healthy': 1645,\n",
       " 'Blueberry___healthy': 1502,\n",
       " 'Cherry_(including_sour)___Powdery_mildew': 1052,\n",
       " 'Cherry_(including_sour)___healthy': 854,\n",
       " 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot': 513,\n",
       " 'Corn_(maize)___Common_rust_': 1192,\n",
       " 'Corn_(maize)___Northern_Leaf_Blight': 985,\n",
       " 'Corn_(maize)___healthy': 1162,\n",
       " 'Grape___Black_rot': 1180,\n",
       " 'Grape___Esca_(Black_Measles)': 1383,\n",
       " 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)': 1076,\n",
       " 'Grape___healthy': 423,\n",
       " 'Orange___Haunglongbing_(Citrus_greening)': 5507,\n",
       " 'Peach___Bacterial_spot': 2297,\n",
       " 'Peach___healthy': 360,\n",
       " 'Pepper,_bell___Bacterial_spot': 997,\n",
       " 'Pepper,_bell___healthy': 1477,\n",
       " 'Potato___Early_blight': 1000,\n",
       " 'Potato___Late_blight': 1000,\n",
       " 'Potato___healthy': 152,\n",
       " 'Raspberry___healthy': 371,\n",
       " 'Soybean___healthy': 5090,\n",
       " 'Squash___Powdery_mildew': 1835,\n",
       " 'Strawberry___Leaf_scorch': 1109,\n",
       " 'Strawberry___healthy': 456,\n",
       " 'Tomato___Bacterial_spot': 2127,\n",
       " 'Tomato___Early_blight': 1000,\n",
       " 'Tomato___Late_blight': 1909,\n",
       " 'Tomato___Leaf_Mold': 952,\n",
       " 'Tomato___Septoria_leaf_spot': 1771,\n",
       " 'Tomato___Spider_mites Two-spotted_spider_mite': 1676,\n",
       " 'Tomato___Target_Spot': 1404,\n",
       " 'Tomato___Tomato_Yellow_Leaf_Curl_Virus': 5357,\n",
       " 'Tomato___Tomato_mosaic_virus': 373,\n",
       " 'Tomato___healthy': 1591}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run through image files and determine if any are the wrong shape\n",
    "# Count images by crop/disease type\n",
    "data_path = 'PlantVillage-Dataset/raw_image_data/color'\n",
    "diff_shape_count = 0\n",
    "img_count = 0\n",
    "leaf_type_img_count = 0\n",
    "leaf_type_img_count_dict = {}\n",
    "for folder in os.listdir(data_path):\n",
    "    for image in os.listdir('%s/%s' % (data_path, folder)):\n",
    "        img_loc = '%s/%s/%s' % (data_path, folder, image)\n",
    "        img = Image.open(img_loc)\n",
    "        arr = np.array(img)\n",
    "        img_shape = arr.shape\n",
    "        img_count += 1\n",
    "        leaf_type_img_count += 1\n",
    "        if img_shape != (256, 256, 3):\n",
    "            diff_shape_count += 1\n",
    "            print(img_loc)\n",
    "            print(img_shape)\n",
    "        else:\n",
    "            continue\n",
    "    leaf_type_img_count_dict[folder] = leaf_type_img_count\n",
    "    leaf_type_img_count = 0\n",
    "print('Wrong Shape Image Count: %d' % (diff_shape_count))\n",
    "print('Image Count: %d' % (img_count))\n",
    "leaf_type_img_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to convert every image into (256*256*3) array\n",
    "def image_to_array(image_loc):\n",
    "    img = Image.open(image_loc)\n",
    "    arr = np.array(img)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to normalize pixels (0 to 1) of each image (0 to 255 pixel values possible)\n",
    "def pixel_normalization(img_array):\n",
    "    img_array = img_array.astype('float32')\n",
    "    img_array /= 255.0\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to center pixel values based on mean pixel value\n",
    "def pixel_centering(norm_img_arr):\n",
    "    mean = norm_img_arr.mean()\n",
    "    norm_img_arr = norm_img_arr - mean\n",
    "    return norm_img_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through image files and convert to pixel array, normalize, and center\n",
    "# Add array to larger data array\n",
    "data_list = []\n",
    "target_list = []\n",
    "data_path = 'PlantVillage-Dataset/raw_image_data/color'\n",
    "for folder in os.listdir(data_path):\n",
    "    for image in os.listdir('%s/%s' % (data_path, folder)):\n",
    "        img_loc = '%s/%s/%s' % (data_path, folder, image)\n",
    "        img_arr = image_to_array(img_loc)\n",
    "        data_list.append(img_arr)\n",
    "        target_list.append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list to array\n",
    "data_array = np.array(data_list)\n",
    "target_array = np.array(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Arrays\n",
    "norm_list = []\n",
    "for img_array in data_array:\n",
    "    norm_arr = pixel_normalization(img_array)\n",
    "    norm_list.append(norm_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize Arrays\n",
    "standardized_list = []\n",
    "for norm_arr in norm_list:\n",
    "    standardized_img_arr = pixel_centering(norm_arr)\n",
    "    standardized_list.append(standardized_img_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to array\n",
    "standardized_data_array = np.array(standardized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(standardized_data_array, target_array,\n",
    "                                                    test_size = .25, random_state = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Train Test Splits with numpy\n",
    "# train_test_dict = {'X_train' : X_train, 'X_test' : X_test, 'y_train' : y_train, 'y_test' : y_test}\n",
    "# for key, val in train_test_dict.items():\n",
    "#     np.save('%s.npy' % (key), val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access numpy objects from bucket\n",
    "# client = storage.Client()\n",
    "# bucket_name = \"capstone-image-classification-bucket\"\n",
    "# bucket = client.get_bucket(bucket_name)\n",
    "# blobs = list(bucket.list_blobs())\n",
    "# for blob in blobs:\n",
    "#     blob.download_to_filename(blob.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Train Test Splits with numpy\n",
    "# X_train = None\n",
    "# X_test = None\n",
    "# y_train = None\n",
    "# y_test = None\n",
    "# train_test_dict = {'X_train' : X_train, 'X_test' : X_test, 'y_train' : y_train, 'y_test' : y_test}\n",
    "# for key in train_test_dict.keys():\n",
    "#     train_test_dict[key] = np.load('%s.npy' % (key))\n",
    "# X_train = train_test_dict['X_train']\n",
    "# X_test = train_test_dict['X_test']\n",
    "# y_train = train_test_dict['y_train']\n",
    "# y_test = train_test_dict['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define epochs, batch size, and number of classes\n",
    "batch_size = 100\n",
    "epochs = 10\n",
    "n_classes = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encode Target Classes\n",
    "target_class_list = list(leaf_type_img_count_dict.keys())\n",
    "le = LabelEncoder()\n",
    "le.fit(target_class_list)\n",
    "target_class_int_list = list(le.classes_)\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target class vectors to target class binary matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes = n_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes = n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct convolutional neural network architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(256,256,3)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40728 samples, validate on 13576 samples\n",
      "Epoch 1/10\n",
      "40728/40728 [==============================] - 660s 16ms/step - loss: 2.6678 - accuracy: 0.3104 - mse: 0.0211 - val_loss: 1.5626 - val_accuracy: 0.5652 - val_mse: 0.0146\n",
      "Epoch 2/10\n",
      "40728/40728 [==============================] - 651s 16ms/step - loss: 1.7351 - accuracy: 0.4999 - mse: 0.0162 - val_loss: 1.3539 - val_accuracy: 0.6164 - val_mse: 0.0134\n",
      "Epoch 3/10\n",
      "40728/40728 [==============================] - 650s 16ms/step - loss: 1.4035 - accuracy: 0.5801 - mse: 0.0139 - val_loss: 0.8312 - val_accuracy: 0.7881 - val_mse: 0.0089\n",
      "Epoch 4/10\n",
      "40728/40728 [==============================] - 650s 16ms/step - loss: 1.2094 - accuracy: 0.6243 - mse: 0.0126 - val_loss: 0.8454 - val_accuracy: 0.7441 - val_mse: 0.0091\n",
      "Epoch 5/10\n",
      "40728/40728 [==============================] - 651s 16ms/step - loss: 1.0660 - accuracy: 0.6598 - mse: 0.0114 - val_loss: 0.6967 - val_accuracy: 0.7889 - val_mse: 0.0078\n",
      "Epoch 6/10\n",
      "40728/40728 [==============================] - 650s 16ms/step - loss: 0.9552 - accuracy: 0.6918 - mse: 0.0104 - val_loss: 0.5731 - val_accuracy: 0.8277 - val_mse: 0.0066\n",
      "Epoch 7/10\n",
      "40728/40728 [==============================] - 650s 16ms/step - loss: 0.8405 - accuracy: 0.7240 - mse: 0.0094 - val_loss: 0.5302 - val_accuracy: 0.8376 - val_mse: 0.0061\n",
      "Epoch 8/10\n",
      "40728/40728 [==============================] - 649s 16ms/step - loss: 0.7397 - accuracy: 0.7527 - mse: 0.0085 - val_loss: 0.5858 - val_accuracy: 0.8214 - val_mse: 0.0066\n",
      "Epoch 9/10\n",
      "40728/40728 [==============================] - 649s 16ms/step - loss: 0.6541 - accuracy: 0.7799 - mse: 0.0076 - val_loss: 0.5044 - val_accuracy: 0.8475 - val_mse: 0.0057\n",
      "Epoch 10/10\n",
      "40728/40728 [==============================] - 651s 16ms/step - loss: 0.6035 - accuracy: 0.7955 - mse: 0.0071 - val_loss: 0.5942 - val_accuracy: 0.8328 - val_mse: 0.0063\n",
      "Test loss: 0.5941627931264687\n",
      "Test accuracy: 0.8327931761741638\n"
     ]
    }
   ],
   "source": [
    "# Compile model and run\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy', 'mse'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size = batch_size,\n",
    "          epochs = epochs,\n",
    "          verbose = 1,\n",
    "          validation_data = (X_test, y_test))\n",
    "score = model.evaluate(X_test, y_test, verbose = 0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
