{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D , Dropout, MaxPooling2D, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL.Image import core as image\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong Shape Image Count: 0\n",
      "Image Count: 54304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Apple___Apple_scab': 630,\n",
       " 'Apple___Black_rot': 621,\n",
       " 'Apple___Cedar_apple_rust': 275,\n",
       " 'Apple___healthy': 1645,\n",
       " 'Blueberry___healthy': 1502,\n",
       " 'Cherry_(including_sour)___Powdery_mildew': 1052,\n",
       " 'Cherry_(including_sour)___healthy': 854,\n",
       " 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot': 513,\n",
       " 'Corn_(maize)___Common_rust_': 1192,\n",
       " 'Corn_(maize)___Northern_Leaf_Blight': 985,\n",
       " 'Corn_(maize)___healthy': 1162,\n",
       " 'Grape___Black_rot': 1180,\n",
       " 'Grape___Esca_(Black_Measles)': 1383,\n",
       " 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)': 1076,\n",
       " 'Grape___healthy': 423,\n",
       " 'Orange___Haunglongbing_(Citrus_greening)': 5507,\n",
       " 'Peach___Bacterial_spot': 2297,\n",
       " 'Peach___healthy': 360,\n",
       " 'Pepper,_bell___Bacterial_spot': 997,\n",
       " 'Pepper,_bell___healthy': 1477,\n",
       " 'Potato___Early_blight': 1000,\n",
       " 'Potato___Late_blight': 1000,\n",
       " 'Potato___healthy': 152,\n",
       " 'Raspberry___healthy': 371,\n",
       " 'Soybean___healthy': 5090,\n",
       " 'Squash___Powdery_mildew': 1835,\n",
       " 'Strawberry___Leaf_scorch': 1109,\n",
       " 'Strawberry___healthy': 456,\n",
       " 'Tomato___Bacterial_spot': 2127,\n",
       " 'Tomato___Early_blight': 1000,\n",
       " 'Tomato___Late_blight': 1909,\n",
       " 'Tomato___Leaf_Mold': 952,\n",
       " 'Tomato___Septoria_leaf_spot': 1771,\n",
       " 'Tomato___Spider_mites Two-spotted_spider_mite': 1676,\n",
       " 'Tomato___Target_Spot': 1404,\n",
       " 'Tomato___Tomato_Yellow_Leaf_Curl_Virus': 5357,\n",
       " 'Tomato___Tomato_mosaic_virus': 373,\n",
       " 'Tomato___healthy': 1591}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run through image files and determine if any are the wrong shape\n",
    "# Count images by crop/disease type\n",
    "data_path = 'PlantVillage-Dataset/raw_image_data/color'\n",
    "diff_shape_count = 0\n",
    "img_count = 0\n",
    "leaf_type_img_count = 0\n",
    "leaf_type_img_count_dict = {}\n",
    "for folder in os.listdir(data_path):\n",
    "    for image in os.listdir('%s/%s' % (data_path, folder)):\n",
    "        img_loc = '%s/%s/%s' % (data_path, folder, image)\n",
    "        img = Image.open(img_loc)\n",
    "        arr = np.array(img)\n",
    "        img_shape = arr.shape\n",
    "        img_count += 1\n",
    "        leaf_type_img_count += 1\n",
    "        if img_shape != (256, 256, 3):\n",
    "            diff_shape_count += 1\n",
    "            print(img_loc)\n",
    "            print(img_shape)\n",
    "        else:\n",
    "            continue\n",
    "    leaf_type_img_count_dict[folder] = leaf_type_img_count\n",
    "    leaf_type_img_count = 0\n",
    "print('Wrong Shape Image Count: %d' % (diff_shape_count))\n",
    "print('Image Count: %d' % (img_count))\n",
    "leaf_type_img_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to convert every image into (256*256*3) array\n",
    "def image_to_array(image_loc):\n",
    "    img = Image.open(image_loc)\n",
    "    arr = np.array(img)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to normalize pixels (0 to 1) of each image (0 to 255 pixel values possible)\n",
    "def pixel_normalization(img_array):\n",
    "    img_array = img_array.astype('float32')\n",
    "    img_array /= 255.0\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to center pixel values based on mean pixel value\n",
    "def pixel_centering(norm_img_arr):\n",
    "    mean = norm_img_arr.mean()\n",
    "    norm_img_arr = norm_img_arr - mean\n",
    "    return norm_img_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through image files and convert to pixel array, normalize, and center\n",
    "# Add array to larger data array\n",
    "data_list = []\n",
    "target_list = []\n",
    "data_path = 'PlantVillage-Dataset/raw_image_data/color'\n",
    "for folder in os.listdir(data_path):\n",
    "    for image in os.listdir('%s/%s' % (data_path, folder)):\n",
    "        img_loc = '%s/%s/%s' % (data_path, folder, image)\n",
    "        img_arr = image_to_array(img_loc)\n",
    "        data_list.append(img_arr)\n",
    "        target_list.append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list to array\n",
    "data_array = np.array(data_list)\n",
    "target_array = np.array(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Arrays\n",
    "norm_list = []\n",
    "for img_array in data_array:\n",
    "    norm_arr = pixel_normalization(img_array)\n",
    "    norm_list.append(norm_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize Arrays\n",
    "standardized_list = []\n",
    "for norm_arr in norm_list:\n",
    "    standardized_img_arr = pixel_centering(norm_arr)\n",
    "    standardized_list.append(standardized_img_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to array\n",
    "standardized_data_array = np.array(standardized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(standardized_data_array, target_array,\n",
    "                                                    test_size = .20, random_state = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Train Test Splits with numpy\n",
    "# train_test_dict = {'X_train' : X_train, 'X_test' : X_test, 'y_train' : y_train, 'y_test' : y_test}\n",
    "# for key, val in train_test_dict.items():\n",
    "#     np.save('%s.npy' % (key), val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access numpy objects from bucket\n",
    "# client = storage.Client()\n",
    "# bucket_name = \"capstone-image-classification-bucket\"\n",
    "# bucket = client.get_bucket(bucket_name)\n",
    "# blobs = list(bucket.list_blobs())\n",
    "# for blob in blobs:\n",
    "#     blob.download_to_filename(blob.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Train Test Splits with numpy\n",
    "# X_train = None\n",
    "# X_test = None\n",
    "# y_train = None\n",
    "# y_test = None\n",
    "# train_test_dict = {'X_train' : X_train, 'X_test' : X_test, 'y_train' : y_train, 'y_test' : y_test}\n",
    "# for key in train_test_dict.keys():\n",
    "#     train_test_dict[key] = np.load('%s.npy' % (key))\n",
    "# X_train = train_test_dict['X_train']\n",
    "# X_test = train_test_dict['X_test']\n",
    "# y_train = train_test_dict['y_train']\n",
    "# y_test = train_test_dict['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define epochs, batch size, and number of classes\n",
    "batch_size = 100\n",
    "epochs = 20\n",
    "n_classes = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encode Target Classes\n",
    "target_class_list = list(leaf_type_img_count_dict.keys())\n",
    "le = LabelEncoder()\n",
    "le.fit(target_class_list)\n",
    "target_class_int_list = list(le.classes_)\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target class vectors to target class binary matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes = n_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes = n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see that GPU is being used\n",
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image generator object\n",
    "img_gen = ImageDataGenerator(width_shift_range = 0.15, height_shift_range = .15, rotation_range = 30, shear_range = .25,\n",
    "                            zoom_range = .25, horizontal_flip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct convolutional neural network architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size = (3, 3), padding = 'same', activation = 'relu', input_shape = (256,256,3)))\n",
    "model.add(MaxPooling2D(pool_size = (3, 3)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding = 'same', activation = 'relu'))\n",
    "model.add(Conv2D(64, (3, 3), padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(128, (3, 3), padding = 'same', activation = 'relu'))\n",
    "model.add(Conv2D(128, (3, 3), padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_classes, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "434/434 [==============================] - 556s 1s/step - loss: 2.7140 - accuracy: 0.2510 - mse: 0.0226 - val_loss: 1.9682 - val_accuracy: 0.4548 - val_mse: 0.0187\n",
      "Epoch 2/20\n",
      "434/434 [==============================] - 560s 1s/step - loss: 1.5960 - accuracy: 0.5363 - mse: 0.0158 - val_loss: 1.3118 - val_accuracy: 0.6308 - val_mse: 0.0136\n",
      "Epoch 3/20\n",
      "434/434 [==============================] - 561s 1s/step - loss: 1.0083 - accuracy: 0.6974 - mse: 0.0109 - val_loss: 0.6136 - val_accuracy: 0.8032 - val_mse: 0.0074\n",
      "Epoch 4/20\n",
      "434/434 [==============================] - 565s 1s/step - loss: 0.7104 - accuracy: 0.7827 - mse: 0.0081 - val_loss: 0.4669 - val_accuracy: 0.8545 - val_mse: 0.0056\n",
      "Epoch 5/20\n",
      "434/434 [==============================] - 565s 1s/step - loss: 0.5482 - accuracy: 0.8301 - mse: 0.0064 - val_loss: 0.4424 - val_accuracy: 0.8647 - val_mse: 0.0053\n",
      "Epoch 6/20\n",
      "434/434 [==============================] - 554s 1s/step - loss: 0.4358 - accuracy: 0.8666 - mse: 0.0052 - val_loss: 0.4996 - val_accuracy: 0.8761 - val_mse: 0.0050\n",
      "Epoch 7/20\n",
      "434/434 [==============================] - 560s 1s/step - loss: 0.3753 - accuracy: 0.8849 - mse: 0.0045 - val_loss: 0.3085 - val_accuracy: 0.9042 - val_mse: 0.0038\n",
      "Epoch 8/20\n",
      "434/434 [==============================] - 554s 1s/step - loss: 0.3344 - accuracy: 0.8973 - mse: 0.0040 - val_loss: 0.2147 - val_accuracy: 0.9319 - val_mse: 0.0027\n",
      "Epoch 9/20\n",
      "434/434 [==============================] - 547s 1s/step - loss: 0.2899 - accuracy: 0.9105 - mse: 0.0035 - val_loss: 0.2279 - val_accuracy: 0.9362 - val_mse: 0.0026\n",
      "Epoch 10/20\n",
      "434/434 [==============================] - 552s 1s/step - loss: 0.2669 - accuracy: 0.9183 - mse: 0.0032 - val_loss: 0.3159 - val_accuracy: 0.9208 - val_mse: 0.0033\n",
      "Epoch 11/20\n",
      "434/434 [==============================] - 560s 1s/step - loss: 0.2349 - accuracy: 0.9269 - mse: 0.0029 - val_loss: 0.4435 - val_accuracy: 0.9113 - val_mse: 0.0038\n",
      "Epoch 12/20\n",
      "434/434 [==============================] - 558s 1s/step - loss: 0.2206 - accuracy: 0.9336 - mse: 0.0027 - val_loss: 0.2199 - val_accuracy: 0.9448 - val_mse: 0.0023\n",
      "Epoch 13/20\n",
      "434/434 [==============================] - 552s 1s/step - loss: 0.2034 - accuracy: 0.9377 - mse: 0.0025 - val_loss: 0.5993 - val_accuracy: 0.9064 - val_mse: 0.0041\n",
      "Epoch 14/20\n",
      "434/434 [==============================] - 557s 1s/step - loss: 0.1898 - accuracy: 0.9437 - mse: 0.0023 - val_loss: 0.2537 - val_accuracy: 0.9380 - val_mse: 0.0026\n",
      "Epoch 15/20\n",
      "434/434 [==============================] - 556s 1s/step - loss: 0.1879 - accuracy: 0.9442 - mse: 0.0022 - val_loss: 0.2083 - val_accuracy: 0.9493 - val_mse: 0.0021\n",
      "Epoch 16/20\n",
      "434/434 [==============================] - 554s 1s/step - loss: 0.1670 - accuracy: 0.9500 - mse: 0.0020 - val_loss: 0.3721 - val_accuracy: 0.9271 - val_mse: 0.0032\n",
      "Epoch 17/20\n",
      "434/434 [==============================] - 570s 1s/step - loss: 0.1594 - accuracy: 0.9520 - mse: 0.0019 - val_loss: 0.3331 - val_accuracy: 0.9360 - val_mse: 0.0028\n",
      "Epoch 18/20\n",
      "434/434 [==============================] - 566s 1s/step - loss: 0.1619 - accuracy: 0.9523 - mse: 0.0019 - val_loss: 0.3788 - val_accuracy: 0.9227 - val_mse: 0.0033\n",
      "Epoch 19/20\n",
      "434/434 [==============================] - 544s 1s/step - loss: 0.1466 - accuracy: 0.9563 - mse: 0.0018 - val_loss: 0.1937 - val_accuracy: 0.9597 - val_mse: 0.0017\n",
      "Epoch 20/20\n",
      "434/434 [==============================] - 544s 1s/step - loss: 0.1408 - accuracy: 0.9588 - mse: 0.0017 - val_loss: 0.1868 - val_accuracy: 0.9606 - val_mse: 0.0017\n"
     ]
    }
   ],
   "source": [
    "# Compile model and run\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy', 'mse'])\n",
    "\n",
    "model_history = model.fit_generator(img_gen.flow(X_train, y_train, batch_size = batch_size),\n",
    "                                    validation_data = (X_test, y_test),\n",
    "                                    steps_per_epoch = len(X_train) // batch_size,\n",
    "                                    epochs = epochs,\n",
    "                                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting metrics over epochs\n",
    "model_acc = model_history.history['acc']\n",
    "model_val_acc = model_history.history['val_acc']\n",
    "model_loss = model_history.history['loss']\n",
    "model_val_loss = model_history.history['val_loss']\n",
    "model_epochs = range(1, len(model_acc) + 1)\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.plot(model_epochs, model_acc, 'darkblue', label = 'Training Accuracy')\n",
    "plt.plot(model_epochs, model_val_acc, 'darkred', label = 'Validation Accuracy')\n",
    "plt.title('Training & Validation Accuracy')\n",
    "plt.legend(loc = 'best')\n",
    "plt.xlabel('# of Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.plot(model_epochs, model_loss, 'darkblue', label = 'Training Loss')\n",
    "plt.plot(model_epochs, model_val_loss, 'darkred', label = 'Validation Loss')\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.legend(loc = 'best')\n",
    "plt.xlabel('# of Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Final Metrics\n",
    "score = model.evaluate(X_test, y_test, verbose = 0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
